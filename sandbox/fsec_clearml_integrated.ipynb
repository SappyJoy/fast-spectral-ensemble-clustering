{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab610a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "# Initialize ClearML task\n",
    "task = Task.init(\n",
    "    project_name=\"Image Classification Project\",\n",
    "    task_name=\"Baseline Experiment\",\n",
    "    task_type=Task.TaskTypes.training\n",
    ")\n",
    "\n",
    "# Example hyperparameters\n",
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "task.connect(params)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse import lil_matrix, coo_matrix, diags\n",
    "from scipy.sparse.linalg import svds, eigsh\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import eigsh\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db769b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9737/1415183348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# Feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clearml/binding/import_bind.py\u001b[0m in \u001b[0;36m__patched_import3\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbase_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msecond_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msecond_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msecond_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         mod = builtins.__org_import__(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clearml/binding/import_bind.py\u001b[0m in \u001b[0;36m__patched_import3\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbase_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msecond_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msecond_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msecond_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         mod = builtins.__org_import__(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from .utils._tags import (\n\u001b[1;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clearml/binding/import_bind.py\u001b[0m in \u001b[0;36m__patched_import3\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbase_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msecond_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msecond_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msecond_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         mod = builtins.__org_import__(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clearml/binding/import_bind.py\u001b[0m in \u001b[0;36m__patched_import3\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbase_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msecond_already_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msecond_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msecond_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         mod = builtins.__org_import__(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/utils/murmurhash.pyx\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Ground truth labels for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c370e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(15, 4))\n",
    "classes = np.unique(y)\n",
    "example_count = 5\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    class_indices = np.where(y == cls)[0][:example_count]\n",
    "    for j, idx in enumerate(class_indices):\n",
    "        ax = axes[j, i]\n",
    "        ax.imshow(data.images[idx], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.set_title(f'Class {cls}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a356195",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35812cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a714bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the data colored by true labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=10)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Data Distribution with True Labels (PCA Reduced)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BKHK(data, num_anchors):\n",
    "    \"\"\"\n",
    "    Balanced K-means-based Hierarchical K-means (BKHK) implementation.\n",
    "    Returns:\n",
    "    - anchors: array of anchor points\n",
    "    - anchor_assignments: array of anchor indices for each sample\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    anchor_assignments = np.zeros(n_samples, dtype=int)\n",
    "    anchors = []\n",
    "    datasets = [(data, np.arange(n_samples), 0)]  # (data, indices, depth)\n",
    "    cluster_id = 0\n",
    "    while len(anchors) < num_anchors and datasets:\n",
    "        new_datasets = []\n",
    "        for subset_data, indices, depth in datasets:\n",
    "            if len(subset_data) <= 1:\n",
    "                anchors.append(subset_data[0])\n",
    "                continue\n",
    "            kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "            labels = kmeans.fit_predict(subset_data)\n",
    "            # Update anchor assignments\n",
    "            anchor_label = cluster_id\n",
    "            anchor_assignments[indices[labels == 0]] = cluster_id\n",
    "            anchors.append(kmeans.cluster_centers_[0])\n",
    "            cluster_id += 1\n",
    "            anchor_assignments[indices[labels == 1]] = cluster_id\n",
    "            anchors.append(kmeans.cluster_centers_[1])\n",
    "            cluster_id += 1\n",
    "            # Add new subsets\n",
    "            idx0 = indices[labels == 0]\n",
    "            idx1 = indices[labels == 1]\n",
    "            data0 = subset_data[labels == 0]\n",
    "            data1 = subset_data[labels == 1]\n",
    "            new_datasets.append((data0, idx0, depth + 1))\n",
    "            new_datasets.append((data1, idx1, depth + 1))\n",
    "            if len(anchors) >= num_anchors:\n",
    "                break\n",
    "        datasets = new_datasets\n",
    "    # Return required number of anchors and assignments\n",
    "    anchors = np.array(anchors[:num_anchors])\n",
    "    return anchors, anchor_assignments\n",
    "\n",
    "# Set the desired number of anchors\n",
    "num_anchors = 45\n",
    "anchors, anchor_assignments = BKHK(X_scaled, num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points and selected anchors\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=10, alpha=0.5, label='Data Points')\n",
    "anchors_pca = pca.transform(anchors)  # Reduce anchors to 2D\n",
    "plt.scatter(anchors_pca[:, 0], anchors_pca[:, 1], c='red', marker='X', s=100, label='Anchors')\n",
    "plt.legend()\n",
    "plt.title(\"Data Distribution with Selected Anchors (PCA Reduced)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anchor_neighbors(anchors, K_prime):\n",
    "    \"\"\"\n",
    "    Compute the K' nearest anchors for each anchor.\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=K_prime + 1, algorithm='auto').fit(anchors)\n",
    "    distances, indices = nbrs.kneighbors(anchors)\n",
    "    # Exclude self (the first neighbor is the anchor itself)\n",
    "    anchor_neighbors = indices[:, 1:]\n",
    "    return anchor_neighbors\n",
    "\n",
    "# Set K' (should be greater than K)\n",
    "K_prime = 13\n",
    "anchor_neighbors = compute_anchor_neighbors(anchors, K_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_anchor_similarities(data, anchors, anchor_assignments, anchor_neighbors, K):\n",
    "    \"\"\"\n",
    "    Compute the similarities between each sample and its K nearest anchors.\n",
    "    Returns:\n",
    "    - W: sparse matrix of shape (n_samples, n_anchors) with similarities\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_anchors = anchors.shape[0]\n",
    "    W = lil_matrix((n_samples, n_anchors))\n",
    "    for i in range(n_samples):\n",
    "        # Find the anchor assignment for this sample\n",
    "        anchor_idx = anchor_assignments[i]\n",
    "        # Candidate anchors are the K' nearest anchors to this anchor\n",
    "        candidate_anchor_indices = anchor_neighbors[anchor_idx]\n",
    "        # Compute distances to candidate anchors\n",
    "        candidate_anchors = anchors[candidate_anchor_indices]\n",
    "        distances = np.linalg.norm(data[i] - candidate_anchors, axis=1)\n",
    "        # Get K nearest anchors\n",
    "        K_nearest_indices = np.argsort(distances)[:K]\n",
    "        K_anchor_indices = candidate_anchor_indices[K_nearest_indices]\n",
    "        K_distances = distances[K_nearest_indices]\n",
    "        # Compute similarities (using normalized inverse distances)\n",
    "        similarities = 1 / (K_distances + 1e-10)\n",
    "        similarities /= similarities.sum()\n",
    "        # Assign to W\n",
    "        W[i, K_anchor_indices] = similarities\n",
    "    return W.tocsr()\n",
    "\n",
    "# Set K (number of nearest anchors to consider)\n",
    "K = 2\n",
    "W = compute_sample_anchor_similarities(X_scaled, anchors, anchor_assignments, anchor_neighbors, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert W to dense for visualization (use a subset for large matrices)\n",
    "subset = 100  # Number of samples and anchors to visualize\n",
    "W_dense = W[:subset, :subset].toarray()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(W_dense, cmap='viridis')\n",
    "plt.title(\"Similarity Matrix Subset (First 100 Samples and Anchors)\")\n",
    "plt.xlabel(\"Anchors\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd(W, n_components):\n",
    "    \"\"\"\n",
    "    Compute the top n_components left singular vectors of W.\n",
    "    Returns:\n",
    "    - U: array of shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    U, Sigma, VT = svds(W, k=n_components)\n",
    "    # Sort the singular values and vectors in descending order\n",
    "    idx = np.argsort(-Sigma)\n",
    "    U = U[:, idx]\n",
    "    return U\n",
    "\n",
    "# Set the number of components (should be greater than the max number of clusters)\n",
    "n_components = 20\n",
    "U = compute_svd(W, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6806ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce spectral embeddings to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "U_tsne = tsne.fit_transform(U)\n",
    "\n",
    "# Plot the spectral embeddings colored by true labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(U_tsne[:, 0], U_tsne[:, 1], c=y, cmap='tab10', s=10)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Spectral Embedding with True Labels (t-SNE Reduced)\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b9e45",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def generate_base_clusterings(U, num_clusters_list):\n",
    "    \"\"\"\n",
    "    Generate base clusterings by running K-means with different numbers of clusters.\n",
    "    Returns:\n",
    "    - base_clusterings: list of arrays of cluster labels\n",
    "    \"\"\"\n",
    "    base_clusterings = []\n",
    "    for k in num_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=None)\n",
    "        labels = kmeans.fit_predict(U)\n",
    "        base_clusterings.append(labels)\n",
    "    return base_clusterings\n",
    "\n",
    "# Define a list of different cluster numbers\n",
    "num_clusters_list = [8, 9, 10, 11, 12]\n",
    "base_clusterings = generate_base_clusterings(U, num_clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 3 base clusterings\n",
    "num_visualize = 3\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(num_visualize):\n",
    "    plt.subplot(1, num_visualize, i+1)\n",
    "    labels = base_clusterings[i]\n",
    "    plt.scatter(U_tsne[:, 0], U_tsne[:, 1], c=labels, cmap='tab10', s=10)\n",
    "    plt.title(f\"Base Clustering {i+1} (k={num_clusters_list[i]})\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86724c03",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def build_bipartite_graph(base_clusterings):\n",
    "    \"\"\"\n",
    "    Build the bipartite graph from base clusterings.\n",
    "    Returns:\n",
    "    - H: incidence matrix (n_samples, n_clusters_total)\n",
    "    \"\"\"\n",
    "    n_samples = base_clusterings[0].shape[0]\n",
    "    cluster_id = 0\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for labels in base_clusterings:\n",
    "        unique_labels = np.unique(labels)\n",
    "        label_mapping = {label: idx + cluster_id for idx, label in enumerate(unique_labels)}\n",
    "        mapped_labels = np.vectorize(label_mapping.get)(labels)\n",
    "        for i in range(n_samples):\n",
    "            rows.append(i)\n",
    "            cols.append(mapped_labels[i])\n",
    "            data.append(1)\n",
    "        cluster_id += len(unique_labels)\n",
    "    n_clusters_total = cluster_id\n",
    "    H = coo_matrix((data, (rows, cols)), shape=(n_samples, n_clusters_total))\n",
    "    return H.tocsr()\n",
    "\n",
    "H = build_bipartite_graph(base_clusterings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548299d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree of each cluster\n",
    "cluster_degrees = np.array(H.sum(axis=0)).flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cluster_degrees, bins=50, kde=True)\n",
    "plt.title(\"Degree Distribution of Clusters in Bipartite Graph\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_clustering(H, n_clusters):\n",
    "    \"\"\"\n",
    "    Perform consensus clustering using spectral clustering on the bipartite graph.\n",
    "    Returns:\n",
    "    - final_labels: array of cluster labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the degree vector of clusters\n",
    "    D_c = np.array(H.sum(axis=0)).flatten()\n",
    "    # Create a sparse diagonal matrix for D_c_inv\n",
    "    D_c_inv = diags(1.0 / D_c)\n",
    "\n",
    "    # Compute the Laplacian L_s = H * D_c^{-1} * H^T\n",
    "    L_s = H.dot(D_c_inv).dot(H.T)\n",
    "\n",
    "    # Ensure L_s is a symmetric matrix (if necessary)\n",
    "    # L_s = (L_s + L_s.T) / 2\n",
    "\n",
    "    # Compute the eigenvectors\n",
    "    n_components = n_clusters\n",
    "    vals, vecs = eigsh(L_s, k=n_components, which='LA')\n",
    "\n",
    "    # Use the eigenvectors for clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    final_labels = kmeans.fit_predict(vecs)\n",
    "    return final_labels\n",
    "\n",
    "# Set the desired number of clusters in the final result\n",
    "final_n_clusters = 10\n",
    "final_labels = consensus_clustering(H, final_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dce54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the final clustering labels for visualization using PCA\n",
    "# Already have U_tsne from earlier\n",
    "\n",
    "# Plot the final clustering\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(U_tsne[:, 0], U_tsne[:, 1], c=final_labels, cmap='tab10', s=10)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Final Clustering Results (t-SNE Reduced)\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73907fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Mutual Information (NMI)\n",
    "nmi = normalized_mutual_info_score(y, final_labels)\n",
    "# Adjusted Rand Index (ARI)\n",
    "ari = adjusted_rand_score(y, final_labels)\n",
    "# Clustering Accuracy (Optional, requires Hungarian Algorithm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy using the Hungarian algorithm.\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in zip(*ind)]) / y_pred.size\n",
    "\n",
    "acc = clustering_accuracy(y, final_labels)\n",
    "\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Clustering Accuracy (ACC): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# True Labels\n",
    "scatter = axes[0].scatter(U_tsne[:, 0], U_tsne[:, 1], c=y, cmap='tab10', s=10)\n",
    "axes[0].legend(*scatter.legend_elements(), title=\"True Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].set_title(\"True Labels (t-SNE Reduced)\")\n",
    "axes[0].set_xlabel(\"t-SNE Component 1\")\n",
    "axes[0].set_ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "# Final Clusters\n",
    "scatter = axes[1].scatter(U_tsne[:, 0], U_tsne[:, 1], c=final_labels, cmap='tab10', s=10)\n",
    "axes[1].legend(*scatter.legend_elements(), title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_title(\"Final Clustering Results (t-SNE Reduced)\")\n",
    "axes[1].set_xlabel(\"t-SNE Component 1\")\n",
    "axes[1].set_ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log visualization of example images\n",
    "Logger.current_logger().report_matplotlib_figure(\"Example Images\", \"Class Distribution\", figure=fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd80cab",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1221c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# param_grid = {\n",
    "#     'num_anchors': np.random.randint(25, 200, 10),\n",
    "#     'K_prime': np.random.randint(2, 20, 5),\n",
    "#     'K': np.random.randint(1, 15, 5),\n",
    "#     'n_components': np.random.randint(10, 40, 5),\n",
    "# }\n",
    "\n",
    "# best_nmi = -1\n",
    "# best_params = {}\n",
    "\n",
    "# for num_anchors, K_prime, K, n_components in product(*param_grid.values()):\n",
    "#     # Run FSEC with these parameters\n",
    "#     anchors, anchor_assignments = BKHK(X_scaled, num_anchors)\n",
    "#     anchor_neighbors = compute_anchor_neighbors(anchors, K_prime)\n",
    "#     W = compute_sample_anchor_similarities(X_scaled, anchors, anchor_assignments, anchor_neighbors, K)\n",
    "#     U = compute_svd(W, n_components)\n",
    "#     base_clusterings = generate_base_clusterings(U, [8, 9, 10, 11, 12])\n",
    "#     H = build_bipartite_graph(base_clusterings)\n",
    "#     final_labels = consensus_clustering(H, n_clusters=10)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     nmi = normalized_mutual_info_score(y, final_labels)\n",
    "    \n",
    "#     if nmi > best_nmi:\n",
    "#         best_nmi = nmi\n",
    "#         best_params = {\n",
    "#             'num_anchors': num_anchors,\n",
    "#             'K_prime': K_prime,\n",
    "#             'K': K,\n",
    "#             'n_components': n_components\n",
    "#         }\n",
    "\n",
    "# print(f\"Best NMI: {best_nmi}\")\n",
    "# print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2a0d9",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a845269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    num_anchors = trial.suggest_int('num_anchors', 25, 100)\n",
    "    K_prime = trial.suggest_int('K_prime', 2, 15)\n",
    "    K = trial.suggest_int('K', 1, 15)\n",
    "    n_components = trial.suggest_int('n_components', 20, 40)\n",
    "    \n",
    "    if num_anchors < n_components:\n",
    "        return 0.0\n",
    "    \n",
    "    if K_prime <= K:\n",
    "        return 0.0\n",
    "    \n",
    "    anchors, anchor_assignments = BKHK(X_scaled, num_anchors)\n",
    "    anchor_neighbors = compute_anchor_neighbors(anchors, K_prime)\n",
    "    W = compute_sample_anchor_similarities(X_scaled, anchors, anchor_assignments, anchor_neighbors, K)\n",
    "    U = compute_svd(W, n_components)\n",
    "    base_clusterings = generate_base_clusterings(U, [8, 9, 10, 11, 12])\n",
    "    H = build_bipartite_graph(base_clusterings)\n",
    "    final_labels = consensus_clustering(H, n_clusters=10)\n",
    "    \n",
    "    nmi = normalized_mutual_info_score(y, final_labels)\n",
    "    return nmi\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=250)\n",
    "\n",
    "print(f\"Best NMI: {study.best_value}\")\n",
    "print(f\"Best Parameters: {study.best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
